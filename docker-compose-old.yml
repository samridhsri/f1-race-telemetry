version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.0
    platform: linux/arm64
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    mem_limit: 256m
    
  kafka:
    image: confluentinc/cp-kafka:7.3.0
    platform: linux/arm64
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_MESSAGE_MAX_BYTES: 10485760  
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760
      KAFKA_FETCH_MESSAGE_MAX_BYTES: 10485760
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
    mem_limit: 1g
    
  mongodb:
    image: mongo:latest
    platform: linux/arm64
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    command: ["--wiredTigerCacheSizeGB", "1.5", "--setParameter", "maxTransactionLockRequestTimeoutMillis=5000"]
    restart: always
    environment:
      MONGO_INITDB_DATABASE: f1db
    mem_limit: 3g
    
  postgres:
    image: postgres:15
    platform: linux/arm64
    container_name: mlflow-postgres
    environment:
      POSTGRES_DB: mlflowdb
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: always
    mem_limit: 512m

  mlflow:
    image: python:3.9-slim
    platform: linux/arm64
    container_name: mlflow-server
    depends_on:
      - postgres
    ports:
      - "5001:5000"
    volumes:
      - mlflow_artifacts:/mlflow/artifacts
      - ./mlflow:/app
    environment:
      - MLFLOW_BACKEND_STORE_URI=postgresql://mlflow:mlflow@postgres:5432/mlflowdb
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/artifacts
      - PYTHONUNBUFFERED=1
    command: ["bash", "/app/start_mlflow.sh"]
    restart: always
    mem_limit: 1g
    
  spark-master:
    image: bitnami/spark:3.3.2
    platform: linux/arm64
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_DAEMON_MEMORY=2g
      - SPARK_WORKER_MEMORY=8g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_DAEMON_MEMORY=512m
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./:/app
      - spark_data:/tmp/spark-events
    depends_on:
      - mongodb
      - kafka
    mem_limit: 2g
    
  spark-worker:
    image: bitnami/spark:3.3.2
    platform: linux/arm64
    container_name: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_DAEMON_MEMORY=2g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./:/app
      - spark_data:/tmp/spark-events
    mem_limit: 5g
      
  spark-processor:
    build:
      context: ./consumer
      dockerfile: Dockerfile.processor
    container_name: spark-processor
    depends_on:
      - mongodb
      - kafka
    volumes:
      - ./consumer:/app
      - /tmp/checkpoint:/tmp/checkpoint
    environment:
      - KAFKA_BROKER=kafka:29092
      - MONGO_URI=mongodb://mongodb:27017/f1db
    restart: on-failure
    
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    platform: linux/arm64
    container_name: api
    ports:
      - "8000:8000"
    depends_on:
      - mongodb
      - kafka
    volumes:
      - ./api:/app
      - ./f1_cache:/app/f1_cache
      - ./producer:/app/producer
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - KAFKA_BROKER=kafka:29092
      - MONGO_URI=mongodb://mongodb:27017/f1db
      - PYTHONUNBUFFERED=1
    restart: always
    mem_limit: 1g
    
    
  producer-builder:
    build:
      context: ./producer
      dockerfile: Dockerfile.producer
    image: producer-image
    entrypoint: ["echo", "Image built: producer-image"]
    
  streamlit:
    build:
      context: ./streamlit
      dockerfile: Dockerfile
    container_name: streamlit
    ports:
      - "8501:8501"
    depends_on:
      - mongodb
      - api
      - mlflow
    volumes:
      - ./streamlit:/app
    restart: always
    environment:
      - PYTHONUNBUFFERED=1
      - API_BASE_URL=http://api:8000
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    mem_limit: 1g
  
    
volumes:
  mongo_data:
  spark_data:
  postgres_data:
  mlflow_artifacts: